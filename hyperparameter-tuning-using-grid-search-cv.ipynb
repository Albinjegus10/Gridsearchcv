{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-10T08:54:50.79663Z","iopub.execute_input":"2022-04-10T08:54:50.797411Z","iopub.status.idle":"2022-04-10T08:54:50.833532Z","shell.execute_reply.started":"2022-04-10T08:54:50.79731Z","shell.execute_reply":"2022-04-10T08:54:50.832545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Please upvote this notebook if you find it useful!**","metadata":{}},{"cell_type":"markdown","source":"# **Hey Guys!**\n\nHere we are going to learn about how to choose best classifier from **Random Forest,Decision Tree,Naive Bias,Support Vector Classification,Logistic Regression,K Neighbours Classifier and SGD Classifier** for our model on wine quality dataset with best and optimal hyperparameters.\n\n**So let's start and get ready to grab new things.**","metadata":{}},{"cell_type":"markdown","source":"# **Let' take an overview of this notebook-**\n\n1.Data Visualisation<br/>\n2.Splitting the dataset<br/>\n3.SVM<br/>\n4.Random Forest<br/>\n5.Logistic Regression<br/>\n6.Guassian Naive Bias<br/>\n7.SGD Classifier<br/>\n8.KNN Classification<br/>\n9.Decision Tree<br/>\n10.Hyperparameter tuning using Grid Search CV<br/>\n11.Classification Report<br/>","metadata":{}},{"cell_type":"markdown","source":"## Importing required libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:50.835353Z","iopub.execute_input":"2022-04-10T08:54:50.835833Z","iopub.status.idle":"2022-04-10T08:54:52.256172Z","shell.execute_reply.started":"2022-04-10T08:54:50.835797Z","shell.execute_reply":"2022-04-10T08:54:52.25549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Overview of Dataset","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/wine-quality-dataset/WineQT.csv')\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:52.260504Z","iopub.execute_input":"2022-04-10T08:54:52.262529Z","iopub.status.idle":"2022-04-10T08:54:52.310361Z","shell.execute_reply.started":"2022-04-10T08:54:52.262489Z","shell.execute_reply":"2022-04-10T08:54:52.30977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:52.314848Z","iopub.execute_input":"2022-04-10T08:54:52.316715Z","iopub.status.idle":"2022-04-10T08:54:52.325978Z","shell.execute_reply.started":"2022-04-10T08:54:52.316677Z","shell.execute_reply":"2022-04-10T08:54:52.325175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:52.330239Z","iopub.execute_input":"2022-04-10T08:54:52.332141Z","iopub.status.idle":"2022-04-10T08:54:52.358826Z","shell.execute_reply.started":"2022-04-10T08:54:52.332104Z","shell.execute_reply":"2022-04-10T08:54:52.358124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### So, there is no null value and all the variables are in the numerical form.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:52.360063Z","iopub.execute_input":"2022-04-10T08:54:52.360294Z","iopub.status.idle":"2022-04-10T08:54:52.402274Z","shell.execute_reply.started":"2022-04-10T08:54:52.360263Z","shell.execute_reply":"2022-04-10T08:54:52.401499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding the number distinct values in variables","metadata":{}},{"cell_type":"code","source":"for i, column in enumerate(df.columns):\n    print(\"{}. \".format(i) + str(column.title()) + \": {}\". format(df[column].nunique()))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:52.403872Z","iopub.execute_input":"2022-04-10T08:54:52.404173Z","iopub.status.idle":"2022-04-10T08:54:52.415647Z","shell.execute_reply.started":"2022-04-10T08:54:52.404139Z","shell.execute_reply":"2022-04-10T08:54:52.414784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the bar graph for target variable 'Quality' and response variable","metadata":{}},{"cell_type":"code","source":"col=['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar','chlorides', 'free sulfur dioxide', \n     'total sulfur dioxide', 'density','pH', 'sulphates', 'alcohol', 'Id']\nfor i in col:\n    fig = plt.figure(figsize = (6,4))\n    sns.barplot(x = 'quality', y = i, data = df)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:52.417173Z","iopub.execute_input":"2022-04-10T08:54:52.417413Z","iopub.status.idle":"2022-04-10T08:54:57.024573Z","shell.execute_reply.started":"2022-04-10T08:54:52.417381Z","shell.execute_reply":"2022-04-10T08:54:57.02365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the above bar graph, we find that there is minimal change in quality w.r.t fixed acidity,residual sugar,density and ph.","metadata":{}},{"cell_type":"markdown","source":"#### Making binary classification for response variable with values between 2 to 6.5 as good and 6.5 to 8 as bad quality using pandas bins and cut method.","metadata":{}},{"cell_type":"code","source":"bins = (2, 6.5, 8) \ngroup_names = ['bad', 'good']\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:57.026041Z","iopub.execute_input":"2022-04-10T08:54:57.0263Z","iopub.status.idle":"2022-04-10T08:54:57.033915Z","shell.execute_reply.started":"2022-04-10T08:54:57.026264Z","shell.execute_reply":"2022-04-10T08:54:57.033111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['quality'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:57.03709Z","iopub.execute_input":"2022-04-10T08:54:57.037407Z","iopub.status.idle":"2022-04-10T08:54:57.0488Z","shell.execute_reply.started":"2022-04-10T08:54:57.03737Z","shell.execute_reply":"2022-04-10T08:54:57.047634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Here we can see that the quality variable is divided into good and bad based on the given limit.","metadata":{}},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:57.050286Z","iopub.execute_input":"2022-04-10T08:54:57.050546Z","iopub.status.idle":"2022-04-10T08:54:57.068465Z","shell.execute_reply.started":"2022-04-10T08:54:57.050513Z","shell.execute_reply":"2022-04-10T08:54:57.067746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Seperating the dataset into target and response variable","metadata":{}},{"cell_type":"code","source":"X = df.drop('quality', axis = 1)\ny = df['quality']","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:57.069783Z","iopub.execute_input":"2022-04-10T08:54:57.070261Z","iopub.status.idle":"2022-04-10T08:54:57.076933Z","shell.execute_reply.started":"2022-04-10T08:54:57.070222Z","shell.execute_reply":"2022-04-10T08:54:57.076187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Splitting the dataset","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:57.078171Z","iopub.execute_input":"2022-04-10T08:54:57.07857Z","iopub.status.idle":"2022-04-10T08:54:57.087059Z","shell.execute_reply.started":"2022-04-10T08:54:57.078529Z","shell.execute_reply":"2022-04-10T08:54:57.086287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying Standard scaling as some of the models gives best result with this feature.","metadata":{}},{"cell_type":"code","source":"sc = StandardScaler()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:57.088237Z","iopub.execute_input":"2022-04-10T08:54:57.088926Z","iopub.status.idle":"2022-04-10T08:54:57.094085Z","shell.execute_reply.started":"2022-04-10T08:54:57.088888Z","shell.execute_reply":"2022-04-10T08:54:57.092951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:57.095478Z","iopub.execute_input":"2022-04-10T08:54:57.09628Z","iopub.status.idle":"2022-04-10T08:54:57.10979Z","shell.execute_reply.started":"2022-04-10T08:54:57.096243Z","shell.execute_reply":"2022-04-10T08:54:57.108979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we train our dataset with different models.\n### So before moving forward lets learn about these models.\n\nAll these comes under supervised machine learning algorithm.</br>\n1.Support Vector Classification - A SVM is a very powerful and versatile ML model capable of performing linear and non linear classification,regression and even outlier detection.These are sensitive to feature scaling.The goal of the SVM is to create hyperplane that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future.SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are called as support vectors, and hence algorithm is termed as Support Vector Machine.SVM can be of two types linear and non-linear.The hyperplane with maximum margin is called the optimal hyperplane.\n\n2.Random Forest - Random Forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset. It is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model.The greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.It takes less training time as compared to other algorithms.It can also maintain accuracy when a large proportion of data is missing.\n\n3.Logistic Regression - Logistic regression uses the concept of predictive modeling as regression; therefore, it is called logistic regression, but is used to classify samples; Therefore, it falls under the classification algorithm.It predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.It uses sigmoid function to classify the data.\n\n4.Gaussian Naive Bias - It is based on Bayes theorem and used for solving classification problems.It is a probabilistic classifier, which means it predicts on the basis of the probability of an object.The Gaussian model assumes that features follow a normal distribution. This means if predictors take continuous values instead of discrete, then the model assumes that these values are sampled from the Gaussian distribution.\n\n5.SGD Classifier-The word ‘stochastic‘ means a system or a process that is linked with a random probability. Hence, in Stochastic Gradient Descent, a few samples are selected randomly called batch instead of the whole data set for each iteration.It basically implements a plain SGD learning routine supporting various loss functions and penalties for classification.\n\n6.KNN Classifier - KNN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.It stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using this algorithm.It is also called a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.Eucledian distance is one of the methods used to calculate nearest neighbours.\n\n7.Decision Tree Classifier-It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.A decision tree simply asks a question, and based on the answer (Yes/No), it further split the tree into subtrees.The decision tree contains lots of layers, which makes it complex.It may have an overfitting issue, which can be resolved using the Random Forest algorithm.","metadata":{}},{"cell_type":"markdown","source":"### Analysis of classification report for each model","metadata":{}},{"cell_type":"code","source":"svc = SVC()\nsvc.fit(X_train, y_train)\npred_svc = svc.predict(X_test)\nprint(classification_report(y_test, pred_svc))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:57.112301Z","iopub.execute_input":"2022-04-10T08:54:57.112509Z","iopub.status.idle":"2022-04-10T08:54:57.152116Z","shell.execute_reply.started":"2022-04-10T08:54:57.11248Z","shell.execute_reply":"2022-04-10T08:54:57.151433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Support vector classifier gets an accuracy of 87%.","metadata":{}},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train, y_train)\npred_rfc = rfc.predict(X_test)\nprint(classification_report(y_test, pred_rfc))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:57.154161Z","iopub.execute_input":"2022-04-10T08:54:57.154676Z","iopub.status.idle":"2022-04-10T08:54:57.6253Z","shell.execute_reply.started":"2022-04-10T08:54:57.154639Z","shell.execute_reply":"2022-04-10T08:54:57.624449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random Forest classifier gets an accuracy of 89%.","metadata":{}},{"cell_type":"code","source":"sgd = SGDClassifier(penalty=None)\nsgd.fit(X_train, y_train)\npred_sgd = sgd.predict(X_test)\nprint(classification_report(y_test, pred_sgd))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:57.626799Z","iopub.execute_input":"2022-04-10T08:54:57.627059Z","iopub.status.idle":"2022-04-10T08:54:57.647564Z","shell.execute_reply.started":"2022-04-10T08:54:57.627024Z","shell.execute_reply":"2022-04-10T08:54:57.64678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### SGD classifier gets an accuracy of 86%.","metadata":{}},{"cell_type":"code","source":"log = LogisticRegression()\nlog.fit(X_train, y_train)\npred_log = log.predict(X_test)\nprint(classification_report(y_test, pred_log))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:57.64894Z","iopub.execute_input":"2022-04-10T08:54:57.649188Z","iopub.status.idle":"2022-04-10T08:54:57.691843Z","shell.execute_reply.started":"2022-04-10T08:54:57.649155Z","shell.execute_reply":"2022-04-10T08:54:57.69115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Logistic Regression gets an accuracy of 89%.","metadata":{}},{"cell_type":"code","source":"NB = GaussianNB()\nNB.fit(X_train, y_train)\npred_NB = NB.predict(X_test)\nprint(classification_report(y_test, pred_NB))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:57.69595Z","iopub.execute_input":"2022-04-10T08:54:57.698266Z","iopub.status.idle":"2022-04-10T08:54:57.729157Z","shell.execute_reply.started":"2022-04-10T08:54:57.698218Z","shell.execute_reply":"2022-04-10T08:54:57.728437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Guassian Naive Bias classifier gets an accuracy of 89%.","metadata":{}},{"cell_type":"code","source":"knn=KNeighborsClassifier()\nknn.fit(X_train, y_train)\npred_knn = knn.predict(X_test)\nprint(classification_report(y_test, pred_knn))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:57.733017Z","iopub.execute_input":"2022-04-10T08:54:57.73576Z","iopub.status.idle":"2022-04-10T08:54:57.789298Z","shell.execute_reply.started":"2022-04-10T08:54:57.735709Z","shell.execute_reply":"2022-04-10T08:54:57.788596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### KNN classifier gets an accuracy of 86%.","metadata":{}},{"cell_type":"code","source":"dtc= DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\npred_dtc = dtc.predict(X_test)\nprint(classification_report(y_test, pred_dtc))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:57.79309Z","iopub.execute_input":"2022-04-10T08:54:57.793981Z","iopub.status.idle":"2022-04-10T08:54:57.817789Z","shell.execute_reply.started":"2022-04-10T08:54:57.793939Z","shell.execute_reply":"2022-04-10T08:54:57.816672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Decision Tree classifier gets an accuracy of 85%.","metadata":{}},{"cell_type":"markdown","source":"### Grid Search CV\n\nGridSearchCV identifies the best technically parameters and models, but only that are included in parameter grid. We do this by defining a dictionary in which we mention a particular hyperparameter along with the values it can take. \n\nIn cross-validation(CV), the process divides the train data further into two parts – the train data and the validation data.\n\n**Here we observe that after applying Grid search CV our best scores increases.**","metadata":{}},{"cell_type":"code","source":"model_params = {\n    'SVM': {\n        'model': SVC(gamma='auto'),\n        'params' : {\n                'C': [0.1,0.8,0.9,1,1.1,1.2,1.3,1.4],\n                'kernel':['linear', 'rbf' ,'poly'],\n                'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]\n        }  \n    },\n    'Random_forest': {\n        'model': RandomForestClassifier(),\n        'params' : {\n            'n_estimators': [1,5,10,20,80,100,140,160,180,200]\n        }\n    },\n    'Logistic_regression' : {\n        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n        'params': {\n            'C': [0.1,0.5,0.8,1,5]\n        }\n    },\n    'Naive_bayes_gaussian': {\n        'model': GaussianNB(),\n        'params': {}\n    },\n    'SGD_classifier': {\n        'model': SGDClassifier(),\n        'params': {}\n    },\n    'KNN_classifier':{\n        'model': KNeighborsClassifier(),\n        'params' : {}\n    },\n    'Decision_tree': {\n        'model': DecisionTreeClassifier(),\n        'params': {\n            'criterion': ['gini','entropy'],\n            \n        }\n    }     \n}","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:57.819063Z","iopub.execute_input":"2022-04-10T08:54:57.819366Z","iopub.status.idle":"2022-04-10T08:54:57.828088Z","shell.execute_reply.started":"2022-04-10T08:54:57.819328Z","shell.execute_reply":"2022-04-10T08:54:57.82721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This is the best scores and best parameters for all the models.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nimport pandas as pd\nscores = []\n\nfor model_name, mp in model_params.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=10, return_train_score=False)\n    clf.fit(X_train,y_train)\n    scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \ndfg = pd.DataFrame(scores,columns=['model','best_score','best_params'])\ndfg","metadata":{"execution":{"iopub.status.busy":"2022-04-10T08:54:57.829353Z","iopub.execute_input":"2022-04-10T08:54:57.830023Z","iopub.status.idle":"2022-04-10T08:57:43.652951Z","shell.execute_reply.started":"2022-04-10T08:54:57.829988Z","shell.execute_reply":"2022-04-10T08:57:43.652291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's run our SVC again with the best parameters.\nsvc2 = SVC(C = 1.2, gamma =  0.8, kernel= 'rbf')\nsvc2.fit(X_train, y_train)\npred_svc2 = svc2.predict(X_test)\nprint(classification_report(y_test, pred_svc2))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T09:04:45.883576Z","iopub.execute_input":"2022-04-10T09:04:45.884096Z","iopub.status.idle":"2022-04-10T09:04:45.952961Z","shell.execute_reply.started":"2022-04-10T09:04:45.884059Z","shell.execute_reply":"2022-04-10T09:04:45.952116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Here we observe that the accuracy of SVC model increases from 87% to 94% using Grid Search CV.","metadata":{}},{"cell_type":"markdown","source":"### Thus, Grid Search CV is a useful technique to identify best and optimal parameters.","metadata":{}},{"cell_type":"markdown","source":"## If you find this notebook helpful, then please upvote as it can be useful for others also.","metadata":{}}]}